% SPDX-FileCopyrightText: 2025 Jorge Teixeira Crespo <jorge.teixeira@udc.es>
%
% SPDX-License-Identifier: GPL-2.0-or-later

\chapter{Implementation}
\label{chap:implementation}

\lettrine{T}{he} deployment of the new infrastructure begins with provisioning a dedicated physical server hosted at Hetzner\cite{hetzner-server-bidding}. This server replaces the previous GPUL machines (\texttt{gpulino} and \texttt{gpulon}), and is named \texttt{gpulux}, reflecting its role as the unified and modernized core system.

\section{Installation and Base System}

The base system was installed using Hetzner's \texttt{installimage}\cite{hetzner-installimage} provisioning tool. The configuration focused on separating system and data storage for flexibility and performance.

Debian was chosen as the operating system, a decision rooted in the board's deep familiarity with the distribution. This choice also honors GPUL's historical ties to Debian and the contributions of past board members who are official Debian maintainers.

\subsection*{installimage Configuration}

The installation was performed on the two 512 GB disks using software RAID 1 to ensure redundancy. The two 2 TB disks were left untouched during provisioning and later configured manually.

The following snippet shows the \texttt{installimage} configuration file:

\begin{lstlisting}[language=bash,caption={Hetzner's installimage script for automated Debian installation on gpulux.}]
## ====================
##  HARD DISK DRIVE(S):
## ====================

DRIVE1 /dev/nvme2n1
DRIVE2 /dev/nvme3n1

## ===============
##  SOFTWARE RAID:
## ===============

SWRAID 1
SWRAIDLEVEL 1

## ==========
##  HOSTNAME:
## ==========

HOSTNAME gpulux

## =============
##  MISC CONFIG:
## =============

USE_KERNEL_MODE_SETTING yes

## ==========================
##  PARTITIONS / FILESYSTEMS:
## ==========================

PART swap  swap   32G
PART /boot ext3  1024M
PART /     ext4   all

## ========================
##  OPERATING SYSTEM IMAGE:
## ========================

IMAGE /root/.oldroot/nfs/install/../images/Debian-1208-bookworm-amd64-base.tar.gz
\end{lstlisting}

This setup results in the following RAID arrays:

\begin{itemize}
  \item \texttt{/dev/md0} - 32 GB swap
  \item \texttt{/dev/md1} - 1 GB \texttt{/boot}
  \item \texttt{/dev/md2} - remaining space (440 GB) for root filesystem
\end{itemize}

\subsection*{Post-Install Configuration}

After installation, the two 2 TB NVMe drives (\texttt{/dev/nvme0n1} and \texttt{/dev/nvme1n1}) were manually configured into a new RAID 1 array, \texttt{/dev/md3}. For security hardening, SSH access for the \texttt{root} user was disabled, and a new admin user, \texttt{tei*****}, was created with \texttt{sudo} privileges. Access for this user is granted via SSH with public key authentication, as password authentication has been disabled server-wide.

\section{Containerization with Incus}

Incus is not included in the standard Debian 12 (Bookworm) repositories, so it must be installed from the \texttt{bookworm-backports} repository. The backports provide newer software versions that are scheduled for inclusion in future Debian releases, in this case, Debian 13 (Trixie).

To enable the backports, a new APT source file is created, including both the official Debian mirror and the Hetzner mirror for optimized download speeds.

\begin{lstlisting}[language=bash,caption={APT sources list to enable the bookworm-backports repository.}]
deb http://mirror.hetzner.com/debian bookworm-backports main
deb http://deb.debian.org/debian bookworm-backports main
\end{lstlisting}

With the backports repository configured, Incus can be installed. Only the \texttt{incus-base} package is installed, as this provides the necessary tools for managing containers without the overhead of full virtual machine support. This approach aligns with the current plan, and support for VMs can be added later if needed.

\begin{lstlisting}[language=bash,caption={Installing the Incus server from Debian backports.}]
sudo apt update
sudo apt install incus-base/bookworm-backports
\end{lstlisting}

To manage Incus, users must be added to the \texttt{incus-admin} group. The administrative user, \texttt{tei*****}, was added to this group.

\subsection*{Initial Incus Configuration}

Incus was initialized using the \texttt{incus admin init} command. The key configuration choices from the interactive setup are summarized below:
\begin{itemize}
    \item \textbf{Mode:} Standalone (non-clustered).
    \item \textbf{Storage Pool:} A new BTRFS storage pool named \texttt{default} was created on the dedicated data RAID array (\texttt{/dev/md3}).
    \item \textbf{Network:} A new local network bridge, \texttt{incusbr0}, was configured with the IPv4 subnet \texttt{10.42.0.1/24} and NAT enabled. IPv6 was disabled for this bridge.
    \item \textbf{Remote Access:} The Incus server is not exposed over the network.
    \item \textbf{Image Management:} Automatic updates for stale cached images were enabled.
\end{itemize}

\section{Monitoring Stack}

With the Incus environment established, the next step is to create containers to host the various services that will run on the new infrastructure. Each service, or group of related services, is isolated within its own container, providing a clean and manageable separation of concerns.

The first container to be created is for the monitoring stack. This stack is a critical component of the new infrastructure, providing insights into the health and performance of the host system and the services it runs.

\subsection*{Monitoring Container}

A new container named \texttt{monitoring} is launched using the official Debian 12 image. The following command is used:

\begin{lstlisting}[language=bash,caption={Creating the monitoring container with a Debian 12 image.}]
incus launch images:debian/12 monitoring
\end{lstlisting}

Once the container is running, it is possible to get an interactive shell inside it to perform administrative tasks. This is achieved with the \texttt{exec} command:

\begin{lstlisting}[language=bash,caption={Obtaining a shell inside the monitoring container.}]
incus exec monitoring -- bash
\end{lstlisting}

Inside the \texttt{monitoring} container, a full monitoring stack is deployed. This stack consists of three key components:

\begin{itemize}
    \item \textbf{Prometheus:} For collecting and storing time-series data (metrics).
    \item \textbf{Loki:} For collecting and storing logs.
    \item \textbf{Grafana:} For visualizing data from both Prometheus and Loki.
\end{itemize}

\subsection*{Prometheus Configuration}

Prometheus was installed following the official documentation\cite{prometheus-getting-started} and configured to scrape both itself and the Incus host metrics:

\begin{lstlisting}[caption={Prometheus configuration to scrape metrics from itself and the Incus host.}]
scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]
  - job_name: "incus"
    metrics_path: '/1.0/metrics'
    scheme: 'https'
    tls_config:
      insecure_skip_verify: true
    static_configs:
      - targets: ['10.42.0.1:8444']
\end{lstlisting}

The host was previously configured to expose metrics without authentication using:

\begin{lstlisting}[language=bash]
incus config set core.metrics_address=:8444
incus config set core.metrics_authentication=false
\end{lstlisting}

\subsection*{Loki, Grafana and Journal Logs Collection}

Loki and Grafana were installed in the \texttt{monitoring} container following their official documentation\cite{grafana-install-debian,loki-storage-retention}. Grafana's configuration is managed via its web UI, so it maintains its default settings post-installation.

Loki was configured to store logs on the container's filesystem in the \texttt{/data/loki} directory. A retention period of 744 hours (approximately one month) was set, after which older logs are automatically purged. It was made accessible from the host using a proxy device, which allows requests from the host's loopback interface to reach the Loki instance:

\begin{lstlisting}[language=bash]
incus config device add monitoring loki-proxy proxy \
  listen=tcp:127.0.0.1:3100 connect=tcp:127.0.0.1:3100
\end{lstlisting}

Grafana Alloy, installed on the host\cite{grafana-alloy-install}, forwards logs from the system journal to Loki. The configuration\cite{grafana-alloy-config-example} extracts key metadata such as unit name, boot ID, transport, and priority level:

\begin{lstlisting}[caption={Grafana Alloy configuration for forwarding journal logs to Loki.}]
loki.source.journal "all" {
  relabel_rules = discovery.relabel.journal_relabel.rules
  forward_to    = [loki.write.local.receiver]
}

discovery.relabel "journal_relabel" {
  targets = []

  rule {
    source_labels = ["__journal__systemd_unit"]
    target_label  = "unit"
  }

  rule {
    source_labels = ["__journal__boot_id"]
    target_label  = "boot_id"
  }

  rule {
    source_labels = ["__journal__transport"]
    target_label  = "transport"
  }

  rule {
    source_labels = ["__journal_priority_keyword"]
    target_label  = "level"
  }
}

loki.write "local" {
  endpoint {
    url = "http://127.0.0.1:3100/loki/api/v1/push"
  }
}
\end{lstlisting}

Additionally, Incus is configured to export internal events (e.g., instance start/stop, snapshot creation) directly to Loki\cite{incus-loki-api}:

\begin{lstlisting}[language=bash]
incus config set loki.api.url=http://127.0.0.1:3100
incus config set loki.instance=incus
\end{lstlisting}

\subsection*{Reverse Proxy Setup with Caddy}

To expose web services to the internet, a lightweight container named \texttt{proxy} was created using Alpine Linux. Caddy was installed to act as a reverse proxy. While it will eventually handle all public-facing services, its initial configuration is limited to exposing the Grafana dashboard.

\begin{lstlisting}[language=bash,caption={Commands to set up the Caddy reverse proxy container.}]
incus launch images:alpine/edge proxy
incus exec proxy -- apk add --no-cache caddy
incus file push Caddyfile proxy/etc/caddy/Caddyfile
incus exec proxy -- rc-service caddy start
incus exec proxy -- rc-update add caddy default

incus config device add proxy http80  proxy \
  listen=tcp:0.0.0.0:80  connect=tcp:127.0.0.1:80
incus config device add proxy http443 proxy \
  listen=tcp:0.0.0.0:443 connect=tcp:127.0.0.1:443
\end{lstlisting}

The last two commands add Incus proxy devices\cite{incus-proxy-device} to the \texttt{proxy} container. These devices forward traffic from a public IP address on the host to a port inside the container. Specifically, all traffic arriving at port 80 (HTTP) and 443 (HTTPS) on the host's public network interface is redirected to the corresponding ports on the container's loopback interface, where Caddy is listening.

The Caddyfile used is as follows:

\begin{lstlisting}[caption={Caddyfile configuration to reverse proxy Grafana.}]
grafana.gpulux.org {
  reverse_proxy monitoring:3000
}
\end{lstlisting}

\subsection*{Grafana Usage}

Grafana was accessed using the configured domain. After creating a user, community dashboards for Prometheus and Incus were imported. The Incus dashboard provides a comprehensive overview of the host's performance, displaying real-time metrics such as CPU and memory usage, network traffic, and disk I/O at project and instance level. This allows for quick identification of resource-intensive instances and overall system health monitoring.

The integration with Loki enables deep exploration of system logs. As shown in Figure~\ref{fig:grafana-journal}, logs from the system journal are collected and enriched with metadata labels. This structured logging is invaluable for debugging and security monitoring, allowing for powerful queries and filtering. The figure shows an example of inspecting SSH authentication failure logs, demonstrating how easily security-relevant events can be isolated and analyzed.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imaxes/grafana-journal.png}
	\caption{Loki logs in Grafana, showing journal entries with custom labels.}
	\label{fig:grafana-journal}
\end{figure}
